{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0033609-ef00-400f-9bcf-e9cecff995f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854fe5b9-8c72-4759-ab54-cd819b8bb49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/lib/python3.10/site-packages/gradio/components/chatbot.py:284: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/gradio/utils.py:1017: UserWarning: Expected 1 arguments for function <function <lambda> at 0x324a883a0>, received 0.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/gradio/utils.py:1021: UserWarning: Expected at least 1 arguments for function <function <lambda> at 0x324a883a0>, received 0.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/gradio/utils.py:1017: UserWarning: Expected 1 arguments for function <function <lambda> at 0x324a8ac20>, received 0.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/gradio/utils.py:1021: UserWarning: Expected at least 1 arguments for function <function <lambda> at 0x324a8ac20>, received 0.\n",
      "  warnings.warn(\n",
      "2025-02-13 19:30:59,491 - INFO - HTTP Request: GET http://localhost:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-02-13 19:30:59,503 - INFO - HTTP Request: HEAD http://localhost:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:31:00,171 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-02-13 19:31:00,343 - INFO - HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://6b9b083f137c6acf25.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:31:02,332 - INFO - HTTP Request: HEAD https://6b9b083f137c6acf25.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6b9b083f137c6acf25.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from typing import List\n",
    "from groq import Groq\n",
    "from pinecone import Pinecone\n",
    "from openai import AzureOpenAI\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# API Configuration\n",
    "AZURE_OPENAI_ENDPOINT = \"https://mobi-dev-openai.openai.azure.com/openai/deployments/insurance-text-embedding-3-small/embeddings?api-version=2023-05-15\"\n",
    "AZURE_OPENAI_API_KEY = \"f5da280ab5fd4f6cb1bcf296b49339f4\"\n",
    "AZURE_OPENAI_API_VERSION = \"2023-05-15\"\n",
    "PINECONE_API_KEY = \"pcsk_4wCzBu_M5yMXdFNFRBL8NZU2XB4GrADRbynuMK7ww1GkSyWt7ER5cimPr1awGis9Hi6563\"\n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "# Initialize clients globally\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"gst-chat-agent\")\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generate embedding for text\"\"\"\n",
    "    try:\n",
    "        response = azure_client.embeddings.create(\n",
    "            input=[text],\n",
    "            model=\"insurance-text-embedding-3-small\"\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Embedding error: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_context(query: str) -> str:\n",
    "    \"\"\"Get relevant context from Pinecone for all queries\"\"\"\n",
    "    try:\n",
    "        embedding = get_embedding(query)\n",
    "        if not embedding:\n",
    "            return \"\"\n",
    "            \n",
    "        results = index.query(\n",
    "            vector=embedding,\n",
    "            top_k=3,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        contexts = []\n",
    "        for match in results.matches:\n",
    "            text = match.metadata.get('text', '').strip()\n",
    "            if text:\n",
    "                contexts.append(text)\n",
    "        \n",
    "        return \"\\n\\n\".join(contexts)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Context error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_chat_response(message: str, chat_history: list) -> str:\n",
    "    \"\"\"Get response from Groq with conversation history\"\"\"\n",
    "    try:\n",
    "        # Get context only if needed\n",
    "        context = get_context(message)\n",
    "        \n",
    "        # Build conversation history\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"You are Glacien's GST expert. Provide clear, accurate answers about Indian GST.\"\"\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Add chat history (limited to last 5 exchanges)\n",
    "        for user_msg, assistant_msg in chat_history[-5:]:\n",
    "            messages.extend([\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_msg}\n",
    "            ])\n",
    "        \n",
    "        # Add current message with context if available\n",
    "        current_message = f\"Context: {context}\\nQuestion: {message}\" if context else message\n",
    "        messages.append({\"role\": \"user\", \"content\": current_message})\n",
    "        \n",
    "        completion = groq_client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Response error: {e}\")\n",
    "        return \"I encountered an error. Please try again.\"\n",
    "\n",
    "def respond(message: str, chat_history: list) -> tuple:\n",
    "    \"\"\"Process message and update chat history\"\"\"\n",
    "    if not message.strip():\n",
    "        return \"\", chat_history\n",
    "    \n",
    "    response = get_chat_response(message, chat_history)\n",
    "    chat_history.append((message, response))\n",
    "    \n",
    "    return \"\", chat_history\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Gradio interface setup\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\"\"\"\n",
    "    .user-message { background-color: #e3f2fd !important; }\n",
    "    .assistant-message { background-color: #f5f5f5 !important; }\n",
    "    .message { margin: 8px 0; }\n",
    "    .chatbot { height: 450px; overflow-y: auto; }\n",
    "    \"\"\"\n",
    ") as demo:\n",
    "    # Header\n",
    "    with gr.Row():\n",
    "        if os.path.exists(\"logo.png\"):\n",
    "            gr.Image(\"logo.png\", show_label=False, height=40)\n",
    "        gr.Markdown(\"# Glacien GST Assistant\")\n",
    "    \n",
    "    gr.Markdown(\"Ask any questions about Indian GST regulations and compliance.\")\n",
    "    \n",
    "    # State for managing chat history\n",
    "    state = gr.State([])\n",
    "    \n",
    "    # Chat interface with persistent history\n",
    "    chatbot = gr.Chatbot(\n",
    "        value=[],\n",
    "        height=450,\n",
    "        show_label=False,\n",
    "        avatar_images=(\"ðŸ‘¤\", \"ðŸ¤–\"),\n",
    "        bubble_full_width=False,\n",
    "        render_markdown=True,\n",
    "        elem_classes={\n",
    "            \"user\": [\"user-message\", \"message\"],\n",
    "            \"bot\": [\"assistant-message\", \"message\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Input area\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(\n",
    "            show_label=False,\n",
    "            placeholder=\"Type your GST question here...\",\n",
    "            scale=8\n",
    "        )\n",
    "        send = gr.Button(\"Send\", scale=1, variant=\"primary\")\n",
    "\n",
    "    # Clear button\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "    \n",
    "    # Event handlers with concurrency handling\n",
    "    def clear_chat():\n",
    "        return [], [], []\n",
    "    \n",
    "    txt.submit(\n",
    "        fn=respond,\n",
    "        inputs=[txt, chatbot],\n",
    "        outputs=[txt, chatbot],\n",
    "        api_name=\"chat\"\n",
    "    ).then(\n",
    "        lambda x: gr.update(interactive=True),\n",
    "        None,\n",
    "        [txt]\n",
    "    )\n",
    "    \n",
    "    send.click(\n",
    "        fn=respond,\n",
    "        inputs=[txt, chatbot],\n",
    "        outputs=[txt, chatbot]\n",
    "    ).then(\n",
    "        lambda x: gr.update(interactive=True),\n",
    "        None,\n",
    "        [txt]\n",
    "    )\n",
    "    \n",
    "    clear.click(\n",
    "        fn=clear_chat,\n",
    "        inputs=None,\n",
    "        outputs=[txt, chatbot, state]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Launch with proper configuration\n",
    "    # demo.queue()  # Enable queuing without the deprecated parameter\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "        share=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a04661-ea12-488b-b388-cd27542ab2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 19:55:22,859 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='{\\n   \"fetch_documents\": true\\n}', role='assistant', function_call=None, reasoning=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Query Classifier System\\nA document retrieval classification system that determines when to fetch relevant documents based on query content.\\nOverview\\nThis system analyzes user queries and returns a JSON response indicating whether document retrieval is required.\\nCore Functionality\\nThe system implements a binary classification:\\n\\nReturns { \\\"fetch_documents\\\": true } for GST-related queries\\nReturns { \\\"fetch_documents\\\": false } for all other queries\\n\\nImplementation Rules\\nResponse Format\\n{ \\\"fetch_documents\\\": true }\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"gst task\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"{\\n   \\\"fetch_documents\\\": true\\n}\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=False,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc73600-4505-4d67-b0cd-62ffa586dba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
